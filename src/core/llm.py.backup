"""
LLM utilities for enhanced report generation.
Supports multiple providers: OpenAI, Perplexity, and Hugging Face Inference API.
"""

import logging
import os
from typing import Optional, Dict, Any

from langchain_openai import ChatOpenAI
from langchain_community.llms import HuggingFaceHub

from src.core.utils import get_huggingface_token, get_openai_api_key

logger = logging.getLogger(__name__)


class LLMProviderFactory:
    """
    Factory for creating provider-aware LLM clients.
    Supports OpenAI, Perplexity, and Hugging Face providers.
    """

    PROVIDER_MODELS = {
        "openai": "gpt-4o-mini",  # Cost-effective, excellent quality
        "perplexity": "llama-3.1-sonar-large-128k-online",  # Real-time web search
        "huggingface": "meta-llama/Llama-3.2-3B-Instruct"  # Good quality, fast, free
    }

    @staticmethod
    def create_client(provider: str, **kwargs) -> Optional[Any]:
        """
        Create an LLM client for the specified provider.

        Args:
            provider: "openai", "perplexity", or "huggingface"
            **kwargs: Additional arguments for the client

        Returns:
            Configured LLM client or None if provider unavailable
        """
        if provider == "openai":
            return LLMProviderFactory._create_openai_client(**kwargs)
        elif provider == "perplexity":
            return LLMProviderFactory._create_perplexity_client(**kwargs)
        elif provider == "huggingface":
            return LLMProviderFactory._create_huggingface_client(**kwargs)
        else:
            logger.error(f"Unknown LLM provider: {provider}")
            return None

    @staticmethod
    def _create_openai_client(api_key: Optional[str] = None, model: Optional[str] = None, **kwargs) -> Optional[ChatOpenAI]:
        """Create OpenAI client"""
        api_key = api_key or get_openai_api_key()
        if not api_key:
            logger.warning("OpenAI API key not found")
            return None

        try:
            model = model or LLMProviderFactory.PROVIDER_MODELS["openai"]
            return ChatOpenAI(
                model=model,
                api_key=api_key,
                temperature=kwargs.get("temperature", 0.7),
                max_tokens=kwargs.get("max_tokens", 1000),
                **kwargs
            )
        except Exception as e:
            logger.error(f"Failed to initialize OpenAI client: {e}")
            return None

    @staticmethod
    def _create_perplexity_client(api_key: Optional[str] = None, model: Optional[str] = None, **kwargs) -> Optional[ChatOpenAI]:
        """Create Perplexity client (uses OpenAI-compatible API)"""
        api_key = api_key or os.getenv("PERPLEXITY_API_KEY")
        if not api_key:
            logger.warning("Perplexity API key not found")
            return None

        try:
            model = model or LLMProviderFactory.PROVIDER_MODELS["perplexity"]
            return ChatOpenAI(
                model=model,
                api_key=api_key,
                openai_api_base="https://api.perplexity.ai",
                temperature=kwargs.get("temperature", 0.7),
                max_tokens=kwargs.get("max_tokens", 1000),
                **kwargs
            )
        except Exception as e:
            logger.error(f"Failed to initialize Perplexity client: {e}")
            return None

    @staticmethod
    def _create_huggingface_client(token: Optional[str] = None, model: Optional[str] = None, **kwargs) -> Optional[HuggingFaceHub]:
        """Create Hugging Face client"""
        token = token or get_huggingface_token()
        if not token:
            logger.warning("Hugging Face token not found")
            return None

        try:
            model = model or LLMProviderFactory.PROVIDER_MODELS["huggingface"]
            return HuggingFaceHub(
                repo_id=model,
                huggingfacehub_api_token=token,
                model_kwargs={
                    "temperature": kwargs.get("temperature", 0.7),
                    "max_length": kwargs.get("max_tokens", 1000),
                }
            )
        except Exception as e:
            logger.error(f"Failed to initialize Hugging Face client: {e}")
            return None


class LLMClient:
    """
    Provider-aware LLM client wrapper.
    Can switch between different providers dynamically.
    """

    def __init__(self, provider: str = "openai", **kwargs):
        """
        Initialize LLM client for specific provider.

        Args:
            provider: "openai", "perplexity", or "huggingface"
            **kwargs: Additional arguments passed to provider client
        """
        self.provider = provider
        self.client = LLMProviderFactory.create_client(provider, **kwargs)

        if self.client:
            logger.info(f"LLM client initialized: {provider}")
        else:
            logger.warning(f"Failed to initialize LLM client for provider: {provider}")
            self.provider = None

    def is_available(self) -> bool:
        """Check if LLM client is available"""
        return self.client is not None

    def generate(self, prompt: str, **kwargs) -> Optional[str]:
        """
        Generate response from LLM.

        Args:
            prompt: Input prompt
            **kwargs: Additional generation parameters

        Returns:
            Generated text or None if unavailable
        """
        if not self.client:
            return None

        try:
            if hasattr(self.client, 'invoke'):  # LangChain ChatOpenAI
                response = self.client.invoke(prompt, **kwargs)
                return response.content if hasattr(response, 'content') else str(response)
            else:  # HuggingFace Hub
                return self.client(prompt, **kwargs)
        except Exception as e:
            logger.error(f"LLM generation failed for {self.provider}: {e}")
            return None


# Backward compatibility function
def get_llm_client(provider: str = "openai", **kwargs) -> LLMClient:
    """
    Get an LLM client for the specified provider.

    Args:
        provider: "openai", "perplexity", or "huggingface"
        **kwargs: Additional arguments for the client

    Returns:
        LLMClient instance
    """
    return LLMClient(provider, **kwargs)


# Legacy function for backward compatibility
def get_llm_client_legacy(openai_key: Optional[str] = None, hf_token: Optional[str] = None, model: Optional[str] = None) -> LLMClient:
    """
    Legacy function for backward compatibility.
    Prefers OpenAI if available, falls back to HuggingFace.
    """
    if openai_key or get_openai_api_key():
        return LLMClient("openai", api_key=openai_key, model=model)
    elif hf_token or get_huggingface_token():
        return LLMClient("huggingface", token=hf_token, model=model)
    else:
        logger.warning("No LLM API credentials found")
        return LLMClient("openai")  # Will fail gracefully
            self.client = ChatOpenAI(
                model=self.model,
                temperature=0.7,
                api_key=self.openai_key,
            )
            self.provider = "openai"
            self.enabled = True
            logger.info(f"LLM client initialized with OpenAI: {self.model}")
        except Exception as e:
            logger.error(f"Failed to initialize OpenAI: {e}")
            self.enabled = False

    def _init_huggingface(self):
        """Initialize Hugging Face client"""
        try:
            from huggingface_hub import InferenceClient

            self.model = self.model or self.DEFAULT_HF_MODEL
            self.client = InferenceClient(token=self.hf_token)
            self.provider = "huggingface"
            self.enabled = True
            logger.info(f"LLM client initialized with HuggingFace: {self.model}")
        except Exception as e:
            logger.error(f"Failed to initialize HuggingFace: {e}")
            self.enabled = False

    def generate(
        self,
        prompt: str,
        max_tokens: int = 500,
        temperature: float = 0.7,
        system_prompt: Optional[str] = None,
    ) -> str:
        """
        Generate text using configured LLM API.

        Args:
            prompt: User prompt
            max_tokens: Maximum tokens to generate
            temperature: Sampling temperature (0.0 = deterministic, 1.0 = creative)
            system_prompt: Optional system instruction

        Returns:
            Generated text
        """
        if not self.enabled:
            logger.warning("LLM not enabled, returning empty string")
            return ""

        try:
            if self.provider == "openai":
                return self._generate_openai(prompt, max_tokens, temperature, system_prompt)
            elif self.provider == "huggingface":
                return self._generate_huggingface(prompt, max_tokens, temperature, system_prompt)
            else:
                return ""
        except Exception as e:
            logger.error(f"LLM generation failed: {e}")
            return ""

    def _generate_openai(
        self, prompt: str, max_tokens: int, temperature: float, system_prompt: Optional[str]
    ) -> str:
        """Generate using OpenAI"""
        from langchain_core.messages import HumanMessage, SystemMessage

        messages = []
        if system_prompt:
            messages.append(SystemMessage(content=system_prompt))
        messages.append(HumanMessage(content=prompt))

        logger.info("ðŸ¤– Generating AI analysis with OpenAI...")
        
        response = self.client.invoke(
            messages,
            max_tokens=max_tokens,
            temperature=temperature,
        )

        logger.debug(f"OpenAI generated {len(response.content)} chars")
        return response.content.strip()

    def _generate_huggingface(
        self, prompt: str, max_tokens: int, temperature: float, system_prompt: Optional[str]
    ) -> str:
        """Generate using Hugging Face"""
        from tqdm import tqdm
        import sys
        
        # Format prompt for chat models
        if system_prompt:
            full_prompt = f"<s>[INST] {system_prompt}\n\n{prompt} [/INST]"
        else:
            full_prompt = f"<s>[INST] {prompt} [/INST]"

        # Show progress bar while waiting for HF API
        logger.info("ðŸ¤— Generating AI analysis with Hugging Face (this may take 30-60 seconds on free tier)...")
        
        with tqdm(total=100, desc="AI Analysis", bar_format='{l_bar}{bar}| {elapsed}', file=sys.stderr) as pbar:
            # Start progress animation
            pbar.update(10)
            
            response = self.client.text_generation(
                full_prompt,
                model=self.model,
                max_new_tokens=max_tokens,
                temperature=temperature,
                return_full_text=False,
            )
            
            pbar.update(90)  # Complete

        logger.debug(f"HuggingFace generated {len(response)} chars")
        return response.strip()

    def generate_regime_summary(
        self,
        symbol: str,
        regime_lt: str,
        regime_mt: str,
        regime_st: str,
        features: dict,
        ccm_notes: Optional[str] = None,
        contradictor_notes: Optional[str] = None,
    ) -> str:
        """
        Generate natural language regime analysis summary.

        Args:
            symbol: Asset symbol
            regime_lt: Long-term regime
            regime_mt: Medium-term regime
            regime_st: Short-term regime
            features: Dict with feature values
            ccm_notes: Cross-asset context notes
            contradictor_notes: Contradictor findings

        Returns:
            Natural language summary
        """
        if not self.enabled:
            return self._fallback_summary(symbol, regime_st)

        system_prompt = """You are a quantitative analyst specializing in crypto market regime analysis.
Provide concise, professional analysis in 2-3 paragraphs. Focus on actionable insights.
Use technical terminology but keep it accessible. Be specific about market conditions."""

        user_prompt = f"""Analyze the following market regime data for {symbol}:

**Multi-Timeframe Regimes:**
- Long-term (1D): {regime_lt}
- Medium-term (4H): {regime_mt}  
- Short-term (15m): {regime_st}

**Key Features:**
- Hurst Exponent (ST): {features.get('hurst_avg', 'N/A'):.2f}
- Variance Ratio (ST): {features.get('vr_statistic', 'N/A'):.2f}
- Volatility (ST): {features.get('returns_vol', 'N/A'):.4f}

**Cross-Asset Context:**
{ccm_notes or 'No cross-asset data available'}

**Contradictor Findings:**
{contradictor_notes or 'No significant contradictions'}

Provide a 2-3 paragraph analysis covering:
1. Current market state and what it means for traders
2. Alignment or divergence across timeframes
3. Key risks and opportunities"""

        return self.generate(
            prompt=user_prompt,
            system_prompt=system_prompt,
            max_tokens=400,
            temperature=0.6,  # Slightly lower for more factual output
        )

    def _fallback_summary(self, symbol: str, regime_st: str) -> str:
        """Fallback when LLM is not available"""
        return f"Market regime analysis for {symbol}: {regime_st}. LLM summary unavailable - set HUGGINGFACE_API_TOKEN for enhanced reports."


# Singleton instance
_llm_client: Optional[LLMClient] = None


def get_llm_client() -> LLMClient:
    """Get or create LLM client singleton"""
    global _llm_client
    if _llm_client is None:
        _llm_client = LLMClient()
    return _llm_client

